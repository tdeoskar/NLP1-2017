{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tharangni H Sivaji - 11611065"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7d17f8efec9d4c7691aaf406e96e878",
     "grade": false,
     "grade_id": "cell-cdf5c1e89cd73689",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 2: Sequence Models\n",
    "\n",
    "In this lab, you will implement a **Hidden Markov Model (HMM)**. HMMs specify a **joint** probability over **observations** and **hidden states**. \n",
    "\n",
    "This is what we will do today:\n",
    "\n",
    "1. **Estimate** a simple HMM model from training data (supervised learning)\n",
    "2. Find the *best* sequence of hidden states for a given sequence of observations (we define \"best\" in 2 different ways!)\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "### Rules\n",
    "* The lab exercises should be made in **groups of two people**.\n",
    "\n",
    "* The deadline is **Tuesday 14 nov 16:59**.\n",
    "\n",
    "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
    "\n",
    "* The **filename** should be `lab2_lastname1_lastname2.ipynb`, so for example `lab2_Manning_Schuetze.ipynb`.\n",
    "\n",
    "* The notebook is graded on a scale of **0-50**. The number of points for each question is indicated in parantheses. \n",
    "\n",
    "* The questions marked **BONUS** give you bonus points; try them if you want an extra challenge\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should **write your code and answers in this iPython Notebook** (see http://ipython.org/notebook.html for reference material). If you have problems, please contact your teaching assistant.\n",
    "\n",
    "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
    "\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
    "    \n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "    \n",
    "* Test your code and **make sure we can run your notebook**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "305e18689b10f62cd47af87aa86d6db5",
     "grade": false,
     "grade_id": "cell-c20720f24702422e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "$ \\Sigma := \\{ o_1, \\dots, o_J \\} $ is our set of **observations**\n",
    "\n",
    "$\\Lambda := \\{c_1, \\dots, c_K \\}$ is our set of **state labels**\n",
    "\n",
    "$\\Sigma^*$ are **all possible sequences** of observations (including empty string $\\epsilon$)\n",
    "\n",
    "$\\Lambda^*$ all possible sequences of hidden states (including empty string $\\epsilon$)\n",
    "\n",
    "> Extra info: we can say that $\\Sigma^*$ is the [Kleene-closure](https://en.wikipedia.org/wiki/Kleene_star) of $\\Sigma$, and $\\Lambda^*$ the Kleene-closure of $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "72ff70df808fef49495dc358e57c44c4",
     "grade": false,
     "grade_id": "cell-e0563c61a6ff0ee4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## A simple example: The Baby HMM\n",
    "\n",
    "We start with a simple example, so that we can easily verify that our code is correct.\n",
    "\n",
    "Consider that we are modeling how a baby behaves. We observe the baby doing the following things: laughing (`laugh`), crying (`cry`), and sleeping (`sleep`). This is our set $\\Sigma$ of **observations**. \n",
    "\n",
    "We presume that, at any moment, the baby can be either `hungry`, `bored`, or `happy`. Since babies cannot talk, each of these states is hidden. This is our set $\\Lambda$ of **hidden states**.\n",
    "\n",
    "**Now the question is: if we have a series of observations, can we predict what hidden states the baby went through?**\n",
    "\n",
    "To tackle this problem, we assume that the baby behaves like a **1st order discrete Markov chain**: the baby's current state only depends on its previous hidden state. The baby can be described as an HMM. (Yay!)\n",
    "\n",
    "For example, assume we observed the baby doing the following:\n",
    "\n",
    "```\n",
    "sleep cry laugh cry\n",
    "cry cry laugh sleep\n",
    "```\n",
    "\n",
    "We will use these sequences as our **test set**. We can try to find out the states of the baby for those observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b68dcbf4fa8b65fec56123b0ecd4b4f1",
     "grade": false,
     "grade_id": "cell-2fe112701eae5d42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, to train our model, we will need some examples of **observations** and **states**; this is our **training set**:\n",
    "\n",
    "```\n",
    "laugh/happy cry/bored cry/hungry sleep/happy\n",
    "cry/bored laugh/happy cry/happy sleep/happy\n",
    "cry/hungry laugh/happy cry/bored sleep/happy\n",
    "```\n",
    "\n",
    "So we have **pairs** observation/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85ab734c791a31b39be5103d2ae24044",
     "grade": false,
     "grade_id": "cell-5a80c3b9c59cfc01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbdc7d0623314b97f193a8f7b3ffeb7",
     "grade": false,
     "grade_id": "cell-ab15ee4ee723c00e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# read in test data\n",
    "test_data = \"\"\"sleep cry laugh cry\n",
    "cry cry laugh sleep\"\"\"\n",
    "\n",
    "def test_reader(test_lines):\n",
    "    for line in test_lines.splitlines():\n",
    "        yield line.split()\n",
    "\n",
    "test_set = list(test_reader(test_data))\n",
    "\n",
    "# read in train data\n",
    "train_data = \"\"\"laugh/happy cry/bored cry/hungry sleep/happy\n",
    "cry/bored laugh/happy cry/happy sleep/bored\n",
    "cry/hungry cry/bored sleep/happy\"\"\"\n",
    "\n",
    "# for convenience, we define a Observation-State pair class\n",
    "Pair = namedtuple(\"Pair\", [\"obs\", \"state\"])\n",
    "Pair.__repr__ = lambda x: x.obs + \"/\" + x.state\n",
    "\n",
    "def train_reader(train_lines):\n",
    "    for line in train_data.splitlines():\n",
    "        # a pair is a string \"observation/state\" so we need to split on the \"/\"\n",
    "        yield [Pair(*pair.split(\"/\")) for pair in line.split()]\n",
    "\n",
    "training_set = list(train_reader(train_data))\n",
    "\n",
    "# print the results\n",
    "print(\"test set (observations):\")\n",
    "for seq in test_set:\n",
    "    print(seq)\n",
    "print(\"\\ntraining set (observation/state pairs):\")\n",
    "for seq in training_set:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d23796f0f266fcc2b4299a149f60382a",
     "grade": false,
     "grade_id": "cell-a98845e6e8374535",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Vocabularies \n",
    "It's going to be very useful if we can map states and observations to integers, so that we can identify them by a number. If we don't do this, then our implementation will be much slower.\n",
    "\n",
    "Make sure you understand what is going on here: every time we look up a observation or state, the `defaultdict` will create a new key (index) if it has not seen that key (state or observation) before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfe9b63aecdf816cc2f177b4ad8348af",
     "grade": false,
     "grade_id": "cell-387cb576fbdc997c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# create mappings from state/obs to an ID\n",
    "state2i = defaultdict(lambda: len(state2i))\n",
    "obs2i = defaultdict(lambda: len(obs2i))\n",
    "\n",
    "for seq in training_set:\n",
    "    for example in seq:\n",
    "        state_id = state2i[example.state]\n",
    "        obs_id = obs2i[example.obs]\n",
    "        \n",
    "print(\"\\nOur vocabularies:\")\n",
    "print(state2i)\n",
    "print(obs2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a743bf86fb1524ec53879b39b24362e",
     "grade": false,
     "grade_id": "cell-2940eafa637320ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The HMM for the first training set sequence looks like this:\n",
    "\n",
    "![hmm-baby-train-1](hmm-baby-train-1.png)\n",
    "\n",
    "Now, we will **estimate** the following probability distributions:\n",
    "\n",
    "- initial: $P( c_k \\,|\\, \\texttt{start})$\n",
    "- transition: $P( c_k \\,|\\, c_l )$\n",
    "- final: $P(\\texttt{stop} \\,|\\, c_k )$\n",
    "- emission: $P( o_l \\,|\\, c_k)$\n",
    "\n",
    "These distributions are all we need. Remember that:\n",
    "\n",
    "- the probability of transitioning to a state $c_k$ only depends on one previous state $c_l$ (1st order Markov assumption).\n",
    "- emitting an observation $o_l$ only depends on the state $c_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5671c62b864f62358d16e532cbb66b47",
     "grade": false,
     "grade_id": "cell-430223fd21b6ce82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Finding the Maximum Likelihood Parameters\n",
    "\n",
    "Now we would like to know what those distributions look like. This is called **estimation**. Given our training data, we count how many times each event occurs and normalize the counts to form proper probability distributions.\n",
    "\n",
    "Let's first do counts for the start probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07ff5fd477e6cbb42357050b47a08a72",
     "grade": false,
     "grade_id": "cell-df74082a4f0715ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can get the number of states and observations from our dictionaries\n",
    "num_states = len(state2i)\n",
    "num_observations = len(obs2i)\n",
    "\n",
    "# this creates a vector of length `num_states` filled with zeros\n",
    "counts_start = np.zeros(num_states)\n",
    "\n",
    "# now we count 1 every time a sequence starts with a certain state\n",
    "# we look up the index for the state that we want to count using the `state2i` dictionary\n",
    "for seq in training_set:\n",
    "    counts_start[state2i[seq[0].state]] += 1.\n",
    "\n",
    "print(counts_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b8ff82f63c481c2a049021ebd0a3147",
     "grade": false,
     "grade_id": "cell-ba32d826a1471a2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We see each state once at the beginning of a sequence in the training set, so that is why we have a count of 1 for each of them.\n",
    "\n",
    "We now **normalize** those counts, so that we obtain a probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "051575a6dd3ebd00917fe95330485768",
     "grade": false,
     "grade_id": "cell-a818b13defb8e711",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# since p_start is a numpy object, we can call sum on it; easy!\n",
    "total = counts_start.sum()\n",
    "\n",
    "# normalize: divide each count by the total\n",
    "p_start = counts_start / total  \n",
    "print('start', '-->', p_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4a329ff77d111ca1472777ececda336",
     "grade": false,
     "grade_id": "cell-efc1949dc9262b12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We now turn to the **transition probabilities** and **stop probabilities**. We count, and then we normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "75852dec815c8c95b7a4d2e7890d02a7",
     "grade": false,
     "grade_id": "cell-c8f2dba45d15d44c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# we can transition from any state to any other state in principle,\n",
    "# so we create a matrix filled with zeros so that we can count any pair of states\n",
    "# in practice, some transitions might not occur in the training data\n",
    "counts_trans = np.zeros([num_states, num_states])\n",
    "\n",
    "# for the final/stop probabilities, we only need to store `num_states` values.\n",
    "# so we use a vector\n",
    "counts_stop = np.zeros(num_states)\n",
    "\n",
    "# now we count transitions, one sequence at a time\n",
    "for seq in training_set:\n",
    "    for i in range(1, len(seq)):\n",
    "        \n",
    "        # convert the states to indexes\n",
    "        prev_state = state2i[seq[i-1].state]\n",
    "        current_state = state2i[seq[i].state]\n",
    "        \n",
    "        # count\n",
    "        counts_trans[current_state, prev_state] += 1.\n",
    "\n",
    "# count final states\n",
    "for seq in training_set:\n",
    "    state = state2i[seq[-1].state]\n",
    "    counts_stop[state] += 1.\n",
    "\n",
    "# print the counts\n",
    "print(\"Transition counts:\")\n",
    "print(counts_trans)\n",
    "\n",
    "print(\"Final counts:\")\n",
    "print(counts_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe66b4f00f774588a37caefc46ef9092",
     "grade": false,
     "grade_id": "cell-68e67a73fdfab5d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we can normalize again. We will need to collect the total counts per state.\n",
    "Take some time to understand that the totals consist of the transition counts AND the final counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80a6f992400cb527f49966d154d1ec07",
     "grade": false,
     "grade_id": "cell-262f16a6645038a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Useful trick: np.sum(m, 0) sums matrix m along the first dimension:\n",
    "print(counts_trans.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cc961e6017a74bddfb243dc56f0c2aaa",
     "grade": false,
     "grade_id": "cell-a2720bcd24a328fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "total_per_state = counts_trans.sum(0) + counts_stop\n",
    "print(\"Total counts per state:\\n\", total_per_state, \"\\n\")\n",
    "\n",
    "# now we normalize\n",
    "# here '/' works one column at a time in the matrix\n",
    "p_trans = counts_trans / total_per_state\n",
    "print(\"Transition probabilities:\\n\", p_trans)\n",
    "\n",
    "# here '/' divides the values in each corresponding index in the 2 vectors\n",
    "p_stop = counts_stop / total_per_state\n",
    "print(\"Final probabilities:\\n\", p_stop, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b577d1f4a96c0bb05e4b812eb156af1e",
     "grade": false,
     "grade_id": "cell-62194d4b1f2fc0bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**So far so good!** Now let's take care of **emission probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we create a matrix to keep track of emission counts\n",
    "# in principle any states can emit any observation\n",
    "# so we need a matrix again\n",
    "counts_emiss = np.zeros([num_observations, num_states])\n",
    "\n",
    "# count\n",
    "for seq in training_set:\n",
    "    for obs, state in seq:\n",
    "        obs = obs2i[obs]\n",
    "        state = state2i[state]\n",
    "        counts_emiss[obs][state] += 1.\n",
    "\n",
    "# normalize\n",
    "p_emiss = counts_emiss / counts_emiss.sum(0)\n",
    "\n",
    "print(\"emission counts:\\n\", counts_emiss)\n",
    "print(\"p_emiss:\\n\", p_emiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c160f990d561c63ae34d26be9101c2cf",
     "grade": false,
     "grade_id": "cell-fc1d7ccaf0a06283",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is a good moment for a sanity check. First, take a look at the training set to see if these probabilities are correct, i.e. check if  for each state $s_k$: $$\\sum_l P(s_l \\,|\\, s_k) = 1.0$$ Note that this includes transitions to \"stop\" state, so you have to take those into account.\n",
    "\n",
    "## Exercise 1: Sanity check (2.5 points)\n",
    "Write a function `sanity_check(...)` that checks if all distributions are correct. If (and only if) it discovers an incorrect distribution, it should throw an **AssertionError**. \n",
    "\n",
    "If you want, you can include some print statements to see what is going on. \n",
    "\n",
    "**[Python hint]**: use python [assert](https://www.tutorialspoint.com/python/assertions_in_python.htm) statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1cf42159750cefa87e6b1197791a83b5",
     "grade": false,
     "grade_id": "sanity-check",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def almost_one(p, eps=1e-3):\n",
    "    return (1.-eps) < p < (1. + eps)\n",
    "\n",
    "def sanity_check(p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eda5dd0c53d312db475c9a18bbb047d",
     "grade": true,
     "grade_id": "sanity-check-grading",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# you can try your function out like this\n",
    "# (do not use this cell for your solution)\n",
    "try:\n",
    "    sanity_check(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    print(\"All good!\")\n",
    "except AssertionError as e:\n",
    "    print(\"There was a problem: %s\" % str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aecd98bde00a14c41176c63e77fefd8b",
     "grade": false,
     "grade_id": "cell-65c6613cf4767c2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Decoding a sequence\n",
    "\n",
    "Ok, so we have estimated a model. Great. Now what? Well, now we can **decode**! \n",
    "\n",
    "Given an observation sequence $o_1, o_2, \\dots, o_N$, we want to find the sequence of hidden states $s^* = s^*_1, s^*_2, \\dots, s^*_N$ that **best** explains those observations.\n",
    "\n",
    "But what does \"best\" mean?\n",
    "\n",
    "1. If we are interested in the best **global** assignment of states to the sequence as a whole, we can use the **Viterbi** algorithm. \n",
    "2. **BONUS** If we care more about minimizing the **local** error of getting each $s_i$ right, we can use **posterior decoding** (also called *max marginal decoding*). *(This is a bonus exercise at the end!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ec81283427d365ff75c31ec00b89a5f",
     "grade": false,
     "grade_id": "cell-91a84fbaef83d741",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Decoding: The Viterbi Algorithm\n",
    "\n",
    "Viterbi gives us the best global hidden state sequence, i.e.:\n",
    "\n",
    "$$ \\begin{array}{lll}\n",
    "s^* &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N \\,|\\, o_1, o_2, \\dots, o_N ) \\\\\n",
    "    &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N, o_1, o_2, \\dots, o_N ) \n",
    "\\end{array}$$\n",
    "\n",
    "To explain Viterbi we will make use of a **trellis**, a kind of graph that shows us the possible states for each time step. \n",
    "\n",
    "For our earlier example, the trellis looks like this:\n",
    "\n",
    "![hmm-baby-train-1](hmm-baby-train-1-trellis.png)\n",
    "\n",
    "Note, that we can now label the edges of this trellis with the following probabilities:\n",
    "\n",
    "- $P_{\\text start}(c_k \\,|\\, \\text{start})$ on the three edges from \"start\"\n",
    "- $P_{\\text stop}(\\text{stop} \\,|\\, c_k)$ on the three edges leading to \"stop\"\n",
    "- $P_{\\text trans}(c_k \\,|\\, c_{l})$ on each remaining edge from state $c_l$ to $c_k$\n",
    "- $P_{\\text emiss}(o \\,|\\, c_k)$ from each state $c_k$, to an observation $o$ made from that state (not shown here)\n",
    "\n",
    "Do you see that our trellis nicely shows the independence assumptions of the HMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d807b72a99805b51f838fc7f4fd91bb",
     "grade": false,
     "grade_id": "cell-64d5cd7e9c55167a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### A Naive approach to get the best sequence\n",
    "\n",
    "To see why the Viterbi algorithm is so useful, we can consider another way to calculate $s^*$:\n",
    "\n",
    "- Iterate over all possible state sequences (all ways to go from `start` to `stop`)\n",
    "    - Calculate the probability for that sequence\n",
    "    - Store the highest probability seen so far and its sequence\n",
    "- Return the sequence that had the maximum probability\n",
    "\n",
    "The problem with this approach is that there are a lot of possible sequences!\n",
    "\n",
    "# Exercise 2: How many possible state sequences are there? (5 points)\n",
    "\n",
    "*This is a theoretical question.* Assume that you have a set $\\Lambda$ of possible states (so there are $|\\Lambda|$ states), and that the observation sequence is of length $N$. Assume there is a transition from any state to any other state.\n",
    "Write down the formula that gives the number of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "67db7d4d507fe44405c224385b56c6ca",
     "grade": true,
     "grade_id": "cell-a3f990e075d01747",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b09086ddeb8060e817315e26c93523a",
     "grade": false,
     "grade_id": "cell-58d4f2277d79da32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### The Viterbi algorithm\n",
    "\n",
    "*We use a slightly different notation here compared to the lecture.*\n",
    "\n",
    "So how do we find the path with the highest score? The idea is that we can use our trellis to represent an **exponential number of paths**. Since we are only interested in the highest-scoring path, for every state at every time step, we only need to keep track of the **highest** probability that can lead us to that state. We can disregard any other paths that lead to that state, since they will for sure not be part of the highest-scoring path.\n",
    "\n",
    "Viterbi uses **dynamic programming**. Here, that means that we will re-use probabilities that we have already computed, so we never have to compute the score for the same sub-problem multiple times.\n",
    "\n",
    "Let's start at the beginning.\n",
    "\n",
    "For the first time step, the **viterbi score** is the transition probability of reaching a state $c_k$ from \"start\", times the probability of emitting the first observation $o_1$ from that state:\n",
    "\n",
    "$$\\text{viterbi}(1, c_k) = P_{\\text start}( c_k \\,|\\, \\text{start}) \\times P_{\\text emiss}(o_1 \\,|\\, c_k)$$\n",
    "\n",
    "So, the Viterbi trellis represents the path with maximum probability in position $i$ when we are in state $y_i$ having observed $o_1, o_2, \\dots, o_i$, the observations up to and including that point.\n",
    "\n",
    "Now that we have the viterbi scores for all states of the first time step in our trellis, we can use the following **recursive formula** to get the scores for all other states, one time step at a time:\n",
    "\n",
    "$$\\text{viterbi}(i, c_k) = \\big( \\max_{c_l \\in \\Lambda} P_{\\text trans}(c_k | c_l) \\times \\text{viterbi}(i-1, c_l) \\big) \\times P_{\\text emiss}(o_i \\,|\\, c_k)$$\n",
    "\n",
    "Finally, for our final state \"stop\" we need to do something special, since there is no observation there:\n",
    "\n",
    "$$\\text{viterbi}(N+1, \\text{stop}) = \\max_{c_l \\in \\Lambda} P_{\\text stop}(\\text{stop} \\,|\\, c_l) \\times \\text{viterbi}(i-1, c_l)  $$\n",
    "\n",
    "This is all we need to know what probability the highest scoring path has! Do you see how the dynamic programming helps us to solve this task efficiently?\n",
    "\n",
    "#### How did we get here?\n",
    "\n",
    "Once we reach the \"stop\" state we know the maximum probability, but we forgot how we got there! If you don't see this immediately, remember that, whenever we computed the viterbi score for a state, we took the maximum over all previous states' viterbi scores, times the transition from those states. But we didn't keep track of which state was actually selected in that \"max\" operation. So now that we are in \"stop\", we don't know how we got there.\n",
    "\n",
    "To solve this, we will use **backpointers**. Whenever we do a $\\max$, we store what state was selected by that max (i.e. the $\\arg\\max$):\n",
    "\n",
    "$$\\text{backtrack}(i, c_k) = \\arg\\max_{c_l \\in \\Lambda} P_{\\text trans}(c_k | c_l) \\times \\text{viterbi}(i-1, c_l)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f154b2c838bce4bd256f4453cdcb91c3",
     "grade": false,
     "grade_id": "cell-61fe883db37093ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Log probabilities\n",
    "\n",
    "Now you know enough to implement Viterbi! But before we start.. Because probabilities tend to get rather small when multiplying, causing numerical instabilities, we will use **log probabilities**. This means that, instead of multiplying, we can now **sum** probabilities, because:\n",
    "\n",
    "$$ \\log(uv) = \\log u + \\log v$$\n",
    "\n",
    "To get the probability of a  path trough our trellis from \"start\" to \"stop\", we can just **sum** the log-probabilities that we encounter. So, finding the best (\"Viterbi\") path means finding the path with the **highest score**.\n",
    "\n",
    "# Exercise 3: convert all probabilities to log-probabilities (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2ca0e9b409962ca3b4e3df50dad91fb5",
     "grade": false,
     "grade_id": "cell-be1182cf116900c3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Note: only run this cell once; otherwise you get NaN values.\n",
    "# If you get NaN values, try running all cells above first.\n",
    "\n",
    "def convert_to_log(p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Convert all probabilities to log-probabilities\n",
    "    \n",
    "    Important: only run this function with normal probabilities as input! \n",
    "    If you run this twice, things will break.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return p_start, p_trans, p_stop, p_emiss\n",
    "\n",
    "\n",
    "print(\"Before:\\n\", p_start, p_trans, p_stop, p_emiss)\n",
    "\n",
    "# do the conversion using your function\n",
    "p_start, p_trans, p_stop, p_emiss = convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "\n",
    "print(\"After:\\n\", p_start, p_trans, p_stop, p_emiss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33c0db8c9073365b5e7f4173a9ca624e",
     "grade": false,
     "grade_id": "cell-dd2a6b04cf0668e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Smoothing\n",
    "\n",
    "Oops! We got a big red warning! What happened? \n",
    "\n",
    "Some probabilities were 0.0, and the log function is not defined for zero, resulting in a **warning**.\n",
    "\n",
    "To prevent the error, we can add a small **smoothing** value to our **counts**, so that we never have a probability of zero.\n",
    "\n",
    "To make things easier, we define a `normalize_all` function below that does all the normalization again,\n",
    "but now adding a small value to all the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4ffdec55019c15b2456b921d3099dbb",
     "grade": false,
     "grade_id": "cell-d511810070a63ddd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize(x, smoothing=0.1, axis=0):\n",
    "    smoothed = x + smoothing\n",
    "    return smoothed / smoothed.sum(axis)\n",
    "\n",
    "def normalize_all(counts_start, counts_trans, counts_stop, counts_emiss, smoothing=0.1):\n",
    "    \"\"\"Normalize all counts to probabilities, optionally with smoothing.\"\"\"\n",
    "    p_start = normalize(counts_start, smoothing=smoothing)\n",
    "    p_emiss = normalize(counts_emiss, smoothing=smoothing)\n",
    "    \n",
    "    counts_trans_smoothed = counts_trans + smoothing\n",
    "    counts_stop_smoothed = counts_stop + smoothing\n",
    "    total_trans_stop = counts_trans_smoothed.sum(0) + counts_stop_smoothed\n",
    "    p_trans = counts_trans_smoothed / total_trans_stop\n",
    "    p_stop = counts_stop_smoothed / total_trans_stop\n",
    "    \n",
    "    return p_start, p_trans, p_stop, p_emiss\n",
    "\n",
    "\n",
    "# normalize with smoothing\n",
    "smoothing = 0.1\n",
    "p_start, p_trans, p_stop, p_emiss = normalize_all(\n",
    "    counts_start, counts_trans, counts_stop, counts_emiss, smoothing=smoothing)\n",
    "\n",
    "# convert to log-probabilities\n",
    "print(\"Smoothed probabilities:\\n\", p_start, p_trans, p_stop, p_emiss)\n",
    "p_start, p_trans, p_stop, p_emiss = convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "print(\"Smoothed log-probabilities:\\n\", p_start, p_trans, p_stop, p_emiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cad9b6d44023dec5c63acc92eb301e3b",
     "grade": false,
     "grade_id": "cell-b6f521724c6db98b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: implement the Viterbi algorithm (40 points)\n",
    "\n",
    "You will now implement the Viterbi algorithm. Complete the function `viterbi(sequence, p_start, p_trans, p_emiss, p_stop)` below.\n",
    "\n",
    "**Input:** sequence ($o_1, ..., o_N$), $P_\\text{start}$, $P_\\text{trans}$, $P_\\text{emiss}$, $P_\\text{stop}$\n",
    "\n",
    "*Forward pass: compute the best path for every end state*\n",
    "\n",
    "- set $\\text{viterbi}(1, c_k)$ for each $c_k$\n",
    "- for $i=2$ to $N$, and for each $c_k$, set $\\text{viterbi}(i, c_k$) and $\\text{backtrack}(i, c_k)$\n",
    "- $\\text{max_prob} = \\max_{c_l} P_{\\text{stop}}(\\text{stop} \\,|\\, c_l) \\times viterbi(N, c_l)$\n",
    "\n",
    "*Backward pass: backtrack to get most likely path*\n",
    "- $\\hat{s}_N = \\arg\\max_{c_l} P_\\text{stop}(\\text{stop} \\,|\\, c_l) \\times viterbi(N, c_l)$\n",
    "- for $i = N-1$ to $1$: $\\hat{s}_i = \\text{backtrack}(i+1, \\hat{s}_{i+1})$\n",
    "\n",
    "**Output:** max_prob, Viterbi path $\\hat{s}_1, \\hat{s}_2, \\dots, \\hat{s}_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ed57dda38a273cac08b404f3b0dcf472",
     "grade": false,
     "grade_id": "cell-08440e87108b522e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute the Viterbi sequence. \n",
    "    Note: you have to use log-probabilities!\n",
    "    \n",
    "    The return value should be a tuple (max_prob, list_of_viterbi_states)\n",
    "    Return:\n",
    "      - best_score (float) the log-probability of the best path\n",
    "      - best_path (int list) the best path as a list of state IDs\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    \n",
    "    # trellis to store Viterbi scores\n",
    "    # we store -inf as our initial scores since log(0)=-inf\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "\n",
    "    # backpointers to backtrack (to remember what prev. state caused the maximum score)\n",
    "    # we initialize with -1 values, to represent a non-existing index\n",
    "    backpointers = -np.ones([length, num_states], dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return best_score, best_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying out Viterbi\n",
    "\n",
    "Once you have implemented the Viterbi algorithm, try it out on the following sequence.\n",
    "\n",
    "Note: to get all points, make sure that the cell below runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7ea6c52e12af0e3bed461f8c2402595",
     "grade": true,
     "grade_id": "cell-42f13fee74e501ec",
     "locked": true,
     "points": 40,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test out your Viterbi-algorithm here.\"\"\"\n",
    "\n",
    "test_sequence = test_set[0]\n",
    "best_score, best_path = viterbi(test_sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "\n",
    "print(best_score)\n",
    "print(best_path)\n",
    "\n",
    "i2state = {v : k for k, v in state2i.items()}\n",
    "print([i2state[i] for i in best_path])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67cebd26aa1e8f5567fceb059957e013",
     "grade": false,
     "grade_id": "cell-6626d09623968520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have reached the end of lab 2.\n",
    "\n",
    "If you want an additional challenge, and to attempt 10 additional (bonus) points, go ahead and try the following sections. Otherwise, you're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08091bd10779d810e2404b6fa16c384d",
     "grade": false,
     "grade_id": "cell-4caecec1fc7924c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Bonus: Posterior Decoding\n",
    "\n",
    "What if we don't care about the global sequence of hidden states, but more about getting **individual** states right? \n",
    "We now aim to find the state with the **highest state posterior** for each position. So what is a state posterior? It is defined as:\n",
    "\n",
    "$$ P( s_i \\,|\\, o_1, o_2, \\dots, o_N) $$\n",
    "\n",
    "It gives the probability, with observation sequence $o_1, o_2, \\dots, o_N$, that the i'th hidden state was $s_i$. \n",
    "\n",
    "The best state $s_i^*$ for that position is then:\n",
    "\n",
    "$$ s_i^* = \\arg\\max_{s_i \\in \\Lambda} P( s_i \\,|\\, o_1, o_2, \\dots, o_N) $$\n",
    "\n",
    "If we calculate this for each position, we are performing posterior decoding. Do you see the difference with Viterbi? Now we choose the hidden states **independenly**, based on the observations $o = o_1, o_2, \\dots, o_N$, one position at a time. \n",
    "\n",
    "#### How to calculate a state posterior?\n",
    "\n",
    "Good question. Calculating the state posterior is not so easy. Remember that we are interested in one specific time step. To choose the best state for this time step, we need to take into account all paths that lead there. You will soon see why. \n",
    "\n",
    "First, let's define a few useful terms.\n",
    "\n",
    "**Sequence posterior**\n",
    "$$ P( s = s_1, s_2, \\dots, s_N \\,|\\, o = o_1, o_2, \\dots, o_N)  = \\frac{P(s, o)}{P(o)}$$\n",
    "\n",
    "To compute this, we need the **likelihood**:\n",
    "\n",
    "$$ P( o = o_1, o_2, \\dots, o_N ) = \\sum_{s} P(o, s)$$\n",
    "\n",
    "To calculate the likelihood, we thus need to sum over **all possible state sequences s**!\n",
    "\n",
    "Since the number of state sequences can grow so quickly, we will do something smarter than summing over all of them.\n",
    "(We used a similar trick with the Viterbi-algorithm!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "692dcef34af0097f6c8d8f6f9b1f88fe",
     "grade": false,
     "grade_id": "cell-2a92a96f1a873e31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Forward and Backward probabilities\n",
    "\n",
    "We can use compute **forward** and **backward** probabilities, which help us compute the likelihood in **linear time**: O(N).\n",
    "\n",
    "Let's first see how they are computed, before we will use them to calculate the likelihood (and finally the state posterior).\n",
    "\n",
    "The forward probability for a position i and a state $c_k$, gives us the probability of being in that state and that position, having observed $o_1, o_2, \\dots, o_i$:\n",
    "\n",
    "$$\\text{forward}(i, c_k) = P(c_k, o_1, o_2, \\dots, o_i)$$\n",
    "\n",
    "Because of the independence assumptions in the HMM, we can calculate the forward probability of position $i$ using the forward probabilities of each state in position $i-1$:\n",
    "\n",
    "$$\\text{forward}(i, c_k) = \\Big( \\sum_{c_l \\in \\Lambda} P_{\\text trans}(c_k \\,|\\, c_l) \\times \\text{forward}(i-1, c_l) \\Big) \\times P_{\\text emiss}(o_i \\,|\\, c_k)  $$\n",
    "\n",
    "And our special cases:\n",
    "\n",
    "$$\\text{forward}(1, c_k) =P_{\\text start}(c_k \\,|\\, \\text{start}) \\times P_{\\text emiss} (o_1 \\, |\\, c_k)$$\n",
    "$$\\text{forward}(N+1, \\text{stop}) = \\sum_{c_l \\in \\Lambda} P_{\\text trans}(\\text{stop} \\,|\\, c_l) \\times \\text{forward}(N, c_l) $$\n",
    "\n",
    "Did you notice that this is almost exactly the same as the Viterbi scores? Instead of taking the maximum, we are now **summing**!\n",
    "\n",
    "**Warning: you cannot \"just sum\" two probabilities in the log-domain!**\n",
    "A brute-force way to do this correctly would be to do $\\log(\\exp(a) + \\exp(b)$, i.e. convert the probabilities back and then sum, and then convert them to the log domain again. But this exposes us to numeric instabilities again! So a better way to do it is using $$\\log(\\exp(a) + \\exp(b)) = a + \\log(1 + \\exp(b − a)) \\qquad \\text{for } a < b$$\n",
    "\n",
    "We are providing a function `logsum()` for you that sums a list of values in the log-domain (using the above strategy).\n",
    "\n",
    "Now that we can compute forward probabilities: did you notice that $\\text{forward}(N+1, \\text{stop})$ gives us the **likelihood**, i.e. $P(o_1, o_2, \\dots, o_N)$? Take a moment to see why.\n",
    "\n",
    "Sadly, this is still not enough to calculate the state posteror. (You probably guessed, since this is calleed the Forward-**Backward** algorithm!). Right now we know the probability of being in state $s_i$ having observed $o_1, o_2, \\dots, o_i$, but we do **not** know the probability of being in that state knowing $o_{i+1}, \\dots, o_N$. This is what the backward probability gives us:\n",
    "\n",
    "$$\\text{backward}(i, c_l) = \\sum_{c_k \\in \\Lambda} P_\\text{trans}(c_k \\,|\\, c_l) \\times \\text{backward}(i+1,c_k) \\times P_\\text{emiss}(o_{i+1} \\,|\\, c_k)$$\n",
    "\n",
    "And our special cases:\n",
    "\n",
    "$$\\text{backward}(N, c_l) = P_\\text{stop}( \\text{stop} \\,|\\, c_l)$$\n",
    "$$\\text{backward}(0, \\text{start}) = \\sum_{c_k \\in \\Lambda} P_{\\text start}(c_k| \\text{start}) \\times \\text{backward}(1, c_k) \\times P_{\\text emiss}(o_1 \\,|\\, c_k)$$\n",
    "\n",
    "Can you see that we can also calculate the likelihood using just backward probabilities? You can find it in $\\text{backward}(0, \\text{start})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "29235871e879ac3d09c28c7f7f8dacd3",
     "grade": false,
     "grade_id": "cell-f68826be76ed9405",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, implement the forward and backward algorithms.\n",
    "\n",
    "The forward function should return two values:\n",
    "\n",
    "- all forward scores for the trellis (length x num_states)\n",
    "- the forward score of the stop state (not part of the trellis)\n",
    "\n",
    "The backward function should similarly return two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1d5510489aed08187d9110c853902bf9",
     "grade": false,
     "grade_id": "cell-dc3ba24656b09924",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logsum(logv):\n",
    "    \"\"\"Sum of probabilities in log-domain.\"\"\"\n",
    "    res = -np.inf\n",
    "    for val in logv:\n",
    "        res = logsum_pair(res, val)\n",
    "    return res\n",
    "\n",
    "def logsum_pair(logx, logy):\n",
    "    \"\"\"\n",
    "    Return log(x+y), avoiding arithmetic underflow/overflow.\n",
    "    \"\"\"\n",
    "    if logx == -np.inf:\n",
    "        return logy\n",
    "    elif logx > logy:\n",
    "        return logx + np.log1p(np.exp(logy-logx))\n",
    "    else:\n",
    "        return logy + np.log1p(np.exp(logx-logy))\n",
    "\n",
    "\n",
    "def forward(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute Forward probabilities.\n",
    "    Note: all probabilities should be log-probabilities.\n",
    "    \n",
    "    Return:\n",
    "      - trellis with forward probabilities, excluding the \"stop\" cell\n",
    "      - the forward probability of the stop cell (this is the log-likelihood!)\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    \n",
    "    # trellis to store forward probabilities\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trellis, log_likelihood\n",
    "\n",
    "\n",
    "def backward(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute Backward probabilities.\n",
    "    Note: all probabilities should be log-probabilities.\n",
    "    \n",
    "    Return:\n",
    "      - trellis with backward probabilities, excluding the \"start\" cell\n",
    "      - the forward probability of the start cell (this is ALSO the log-likelihood!)\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    \n",
    "    # trellis to store forward probabilities\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trellis, log_likelihood\n",
    "\n",
    "\n",
    "def forward_backward(sequence):\n",
    "    \"\"\"\n",
    "    Compute forward and backward probabilities.\n",
    "    Return:\n",
    "    - fw_trellis\n",
    "    - fw_log_likelihood (the value of the \"stop\" cell, not part of the trellis)\n",
    "    - bw_trellis\n",
    "    - bw_log_likelihood (the value of the \"start\" cell, not part of the trellis)\n",
    "    \"\"\"\n",
    "    fw_trellis, fw_ll = forward(sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    bw_trellis, bw_ll = backward(sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    return fw_trellis, fw_ll, bw_trellis, bw_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31a98f800c90c1bb7b15837ef84f60d3",
     "grade": true,
     "grade_id": "cell-aca305faec766813",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sequence = test_set[0]\n",
    "\n",
    "fw_trellis, fw_ll, bw_trellis, bw_ll = forward_backward(test_sequence)\n",
    "\n",
    "print(test_sequence)\n",
    "print(fw_trellis)\n",
    "print(fw_ll)\n",
    "print(bw_trellis)\n",
    "print(bw_ll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Decoding\n",
    "\n",
    "Implement a function that, given the forward and backward probabilities, and a sequence of observations $o_1, o_2, \\dots, o_N$, returns the best hidden state sequence, according to posterior decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4d9fb8cec58c06ed25bfa556b1082ea8",
     "grade": false,
     "grade_id": "cell-64eb3cc7cc528b10",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def posterior_decode(sequence, fw_trellis, bw_trellis, ll, p_trans, p_emiss):\n",
    "    \"\"\"\n",
    "    Return best hidden state sequence according to Posterior decoding\n",
    "    \"\"\"\n",
    "\n",
    "    length = len(sequence)\n",
    "    num_states = fw_trellis.shape[1]\n",
    "        \n",
    "    # calculate the state posteriors\n",
    "    state_posteriors = np.zeros([length, num_states])\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    state_posteriors = np.exp(state_posteriors)\n",
    "    \n",
    "    # the best states are simply the arg max of the state posteriors\n",
    "    best_sequence = np.argmax(state_posteriors, axis=1)\n",
    "\n",
    "    return state_posteriors, best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694d7758a879137e5cc5442daa984d80",
     "grade": true,
     "grade_id": "cell-7adb0cac6740f307",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "state_posteriors, best_sequence = posterior_decode(test_sequence, fw_trellis, bw_trellis, fw_ll, p_trans, p_emiss)\n",
    "\n",
    "print(state_posteriors)\n",
    "print(best_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
